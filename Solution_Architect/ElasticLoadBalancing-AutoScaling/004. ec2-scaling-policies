Amazon EC2 Auto Scaling Policies

Amazon EC2 Auto Scaling provides  policies to dynamically or predictably adjust the number of instances in an Auto Scaling group (ASG) 
based on demand or predefined schedules. There are four main types of scaling policies:

Target Tracking Scaling (Recommended by AWS)
Simple Scaling
Step Scaling
Scheduled Scaling



Target Tracking Scaling (Recommended)

How it Works: Automatically adjusts the number of instances to maintain a target metric value, such as "average" CPU utilization.

Example:
ASG has 4 instances reporting an average CPU utilization of 71.5%.

Target metric: CPU utilization set to 60%.
The Auto Scaling group will launch additional instances to bring the average CPU utilization closer to the target.

Key Features:
Instances are considered active after their warm-up period expires (time needed for the instance to become operational).

Recommendation: Use metrics with a 1-minute frequency for better responsiveness.



Simple Scaling

How it Works: Adds or removes instances when a specific alarm (e.g., from CloudWatch) is triggered.

Example:
Alarm: CPU utilization ≥ 60%.
When the threshold is breached (e.g., utilization reaches 70%), CloudWatch triggers the scaling action.
After launching new instances, the system waits for 300 seconds (default cooldown period) before allowing further scaling.

Limitations:
Fixed response (e.g., adding a set number of instances) regardless of the severity of the breach.



Step Scaling

How it Works: Similar to Simple Scaling but adjusts the number of instances based on the magnitude of the alarm breach.

Example:
Alarm: CPU utilization ≥ 60%.
Breach Severity:
If CPU utilization = 70%, 2 instances are launched.
If CPU utilization = 80%, 4 instances are launched.
Scaling actions are defined for specific breach ranges.


Scheduled Scaling

How it Works: Launches or terminates a predefined number of instances at a specific time.

Example:
At 8:45 AM, launch enough instances so that they are ready by 9:00 AM to handle increased traffic.

Use Case: Ideal for predictable traffic patterns, such as planned high traffic during specific times of the day (e.g., office hours, marketing campaigns).




Key Points
Target Tracking is AWS's recommended scaling policy due to its flexibility and automation.
Step Scaling allows fine-grained scaling responses based on the size of the breach.
Simple Scaling provides basic scaling but may not handle rapid changes in traffic effectively.
Scheduled Scaling is suitable for predictable workloads but lacks real-time responsiveness.



******************
important keywords:
Alarm
An alarm is a rule set up in Amazon CloudWatch to monitor a specific metric. When the metric exceeds or falls below a defined threshold, the alarm triggers an
action, such as scaling up or down.

Target Metric
A target metric is a measurable parameter that the scaling policy monitors and attempts to maintain at a specific value or range. This metric is 
typically gathered by Amazon CloudWatch and is associated with the performance of the resources in an Auto Scaling Group (ASG).

Aspect	      Target Metric	                              Alarm
Definition	  A performance parameter to maintain.    	  A monitoring rule for specific metrics.
Purpose	      Defines the goal for a scaling policy.	    Triggers actions when thresholds are breached.
Example	      Maintain 60% average CPU utilization.	      Trigger scaling when CPU ≥ 80%.


A metric is a quantifiable measure used to monitor and evaluate the performance or health of a resource in AWS. Metrics are tracked over time
and used to trigger alarms or scaling actions.

Example Metrics:
CPU utilization (e.g., percentage of CPU used by an EC2 instance).
Network traffic (e.g., bytes sent and received).
Disk I/O (e.g., read/write operations).
Request count per target (used with load balancers).

Metrics are collected by AWS services like CloudWatch, which monitors and displays them in real-time or over specific periods.




What Is Application Load Balancer Request Count Per Target?
Application Load Balancer (ALB) distributes incoming application traffic across multiple targets (e.g., EC2 instances) in one or more Availability Zones.

Request Count Per Target:
This metric measures the number of requests that each target (e.g., an EC2 instance in a target group) receives.
It helps ensure that traffic is evenly distributed and that no single target is overwhelmed.
If the number of requests exceeds a defined threshold, the system can automatically scale out by adding more instances to handle the load.

Example Scenario:
Threshold Defined: If "Request Count Per Target" > 50, the Auto Scaling group will add more EC2 instances.
Behavior:
Traffic spikes lead to an increase in requests per target.
The load balancer monitors the metric and triggers a scale-out action when the threshold is breached.
Once traffic subsides, the system scales back in (terminates instances) when requests per target drop below a lower threshold (e.g., 45).

Why It’s Important:
Ensures the application remains responsive during high demand by adding capacity dynamically.
Prevents over-provisioning by scaling in when demand decreases.
Helps maintain a seamless user experience with consistent performanc


Threshold: The metric value must exceed 50 (not equal to or below it).
3 Data Points: All 3 consecutive data points within the 3-minute window must meet or exceed the threshold.

Scenario Analysis:
Minute	Data Point Value	Condition Met?
1	      45               	No (below 50)
2	      49	              No (below 50)
3	      51	              Yes (equal to 50)
4       40                No

For the alarm to trigger, you need 3 consecutive data points where the values are greater than 50. Even though the third minute has a value of 51, 
the first two data points (45 and 49) are below the threshold, so the condition isn't met
************




HOL:




Notes on Configuring and Testing Auto Scaling Policies with AWS EC2 and Load Balancers
1. Initial Setup
Auto Scaling Group: Initially configured with two instances and a load balancer.
Objective: Dynamically scale the number of instances based on demand.

2. Adjusting Auto Scaling Group
Increase Maximum Desired Capacity: Updated the max desired capacity to 4 from 2.
Add Availability Zones:
Edited the network configuration to include additional zones (e.g., us-east-1c and us-east-1d) for improved availability.
Updated the load balancer’s subnets to match the Auto Scaling Group configuration.

3. Creating a Dynamic Scaling Policy
Type of Policy: Target Tracking Scaling.
Metric Used: Application Load Balancer (ALB) Request Count Per Target.
Set a threshold: Scale out when the request count exceeds 50 connections per target.
Scaling In and Out:
Scaling Out(adds the no od resources): Triggered after 3 data points (in 3 minutes) exceed the threshold.
Scaling In(reduces the no of resoruces): Slower, triggered when the request count drops below 45 connections per target (15 data points over 15 minutes).

((Cost Efficiency: Scaling in ensures you're not paying for unused resources during low demand, while scaling out maintains application performance during high demand.))


4. Monitoring Alarms in CloudWatch
Automatic Alarm Creation:
High Alarm: Triggered when request count exceeds the threshold.
Monitored in the CloudWatch Alarms section.
Alarms initially show "Insufficient Data" until enough data is collected.
Load Generation:
Used a curl command in Cloud Shell to generate 200+ connections to the load balancer.
Verified load through CloudWatch metrics and the load balancer monitoring tab.

# Command to generate load on the ALB

***replace with your alb dns name***
```for i in {1..200}; do curl http://your-alb-address.com & done; wait```

5. Observing Scaling Behavior
Auto Scaling Group Activity:
Detected spikes in request counts.
Automatically launched additional instances to meet demand.
Instance Health:
New instances registered in the target group after a warm-up period.
Verified healthy instances in the load balancer.


6. Testing the Setup
Accessed the load balancer's DNS in a browser:
Observed load distribution across four instances.
Verified instances spread across multiple availability zones.


7. Cleanup
Deleted resources to avoid unnecessary charges:
Terminated the Auto Scaling Group (triggered connection draining for instances).
Deleted the load balancer.
Left resources that don’t incur costs (e.g., launch template, target group).



Key Learnings
Scaling Policies:
Dynamic scaling adjusts resources based on real-time metrics.
Target tracking is efficient for maintaining a steady state.
Monitoring:
Use CloudWatch to track performance and ensure scaling behavior aligns with expectations.
Cost Management:
Always clean up unused resources to minimize costs.


