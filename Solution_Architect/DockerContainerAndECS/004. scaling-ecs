

********************************************
WE HAVE GOT A TWO AZ AND WITHIN AZ WE GOT A CONTAINER INSTANCE INSIDE THE AZ AND WITHIN CONTAINER INSTANCE WE GOT MULTIPLE SERVICES SPANING ACROSS TWO AZ.
AND WE GOT A CONTAINER INSIDE THE SERVICE WITH THE PORT ADDRESS AND WITHIN CONTAINER WE GOT TASKS ..

In the context of Amazon ECS (Elastic Container Service), a container instance typically refers to an EC2 instance that is used to run containers.

Container Instance (EC2 instance):

In ECS, a container instance is an EC2 instance that has the Amazon ECS container agent installed and is part of an ECS cluster.
It provides the compute resources (CPU, memory, storage) for running your containers.
These instances can run Docker containers as part of your ECS workloads.
The container instances are registered to an ECS cluster and managed by ECS.

Why EC2?
EC2 instances act as the infrastructure for your containers when you're using ECS with EC2 launch type. 
These EC2 instances are provisioned 
and managed by you or the ECS service, depending on your setup.

Alternative: AWS Fargate

With Fargate, a serverless compute engine, you don't need to manage EC2 instances directly. Fargate abstracts the container instances (EC2) for you
and automatically provisions the required compute resources to run containers.

ECS with EC2 vs. Fargate:
ECS with EC2: You manage and maintain the EC2 instances that act as container instances.
ECS with Fargate: AWS automatically manages the infrastructure (no need to manage EC2 instances yourself).

Example:

If you're using ECS with EC2:
You launch EC2 instances (container instances) and register them in your ECS cluster.
ECS schedules containers to run on these EC2 instances based on resource availability.

If you're using ECS with Fargate:
You donâ€™t have to worry about EC2 instances at all. You just define your task (container) configuration, and Fargate takes care
of provisioning the compute resources for your containers.
So, container instances in ECS typically refer to EC2 instances that run Docker containers, but this changes if you're using 
Fargate for serverless container orchestration.



WHAT EXACTLY IS A CLUSTER?

A cluster is a group of computing resources that work together to achieve a common goal, such as running applications or workloads. 
In the context of containerized environments (like Kubernetes or ECS), a cluster typically refers to a set of machines (physical or virtual) 
that are configured to run containers, orchestrate them, and ensure high availability, scalability, and fault tolerance.

Cluster in Different Contexts:

1. Kubernetes Cluster:
A Kubernetes cluster is a set of nodes (machines, either virtual or physical) that run containerized applications.
It consists of two main components:
Control Plane (Master Node): Manages the cluster's overall state, such as scheduling containers, monitoring their health, and handling the cluster's API.
Worker Nodes: These are the machines that run your application containers (within pods).
A Kubernetes cluster provides features like auto-scaling, self-healing, and load balancing for applications.

2. Amazon ECS Cluster:
In Amazon ECS (Elastic Container Service), a cluster is a logical grouping of container instances (EC2 instances) or Fargate tasks.
The ECS cluster manages and schedules containerized applications (tasks and services) to run on the available resources in the cluster.
A cluster can consist of:
EC2 instances (if you're using the EC2 launch type for ECS).
Fargate (if you're using the serverless option for ECS, where AWS manages the infrastructure).

3. Cluster Benefits:
High Availability: If one instance or machine fails, others in the cluster can take over, keeping the system running.
Scalability: You can scale the cluster by adding more machines or nodes to handle increased workloads.
Resource Management: The cluster manages resources efficiently and allocates them based on demand.
Key Points:
A cluster can be thought of as a group of machines or resources working together as a single unit to manage and run workloads.
In container orchestration, a cluster enables efficient deployment, scaling, and management of containers across multiple machines.

Example:
Kubernetes Cluster: If you're deploying a web application using Kubernetes, your Kubernetes cluster will consist of multiple nodes (machines) that
run your containerized web application across them. If one node fails, Kubernetes will reschedule the application to other healthy nodes in the cluster.

ECS Cluster: If you use ECS, your ECS cluster might contain several EC2 instances that host and run the containers for your web application. You 
can manage this cluster to ensure the application scales with demand and stays available.

In short, a cluster is the foundation for deploying and running applications in a distributed, scalable, and highly available way, 
whether you're using Kubernetes or Amazon ECS.

_________
Define Task Definition
In ECS, we define a task definition that contains both containers (front-end and back-end).
Each container has an internal port that it listens to.
Front-end container (NGINX): It runs on port 80 internally.
Back-end container (Node.js API): It runs on port 3000 internally

{
  "family": "webapp-task",
  "containerDefinitions": [
    {
      "name": "frontend-container",
      "image": "nginx:latest",
      "portMappings": [
        {
          "containerPort": 80,
          "hostPort": 8080
        }
      ]
    },
    {
      "name": "backend-container",
      "image": "node:latest",
      "portMappings": [
        {
          "containerPort": 3000,
          "hostPort": 9090
        }
      ]
    }
  ]
}

ECS Service
When creating an ECS service, we specify how many copies of the task (with both containers) should run. For example, you might want 3 tasks running to handle scaling.


aws ecs create-service --cluster webapp-cluster --service-name webapp-service --task-definition webapp-task --desired-count 3
ECS will run 3 tasks, each containing the front-end container and the back-end container.
ECS will load balance the traffic to the correct container ports using the host ports (8080 and 9090).



How ECS Handles Host Ports in this Scenario:
Assume you have 3 tasks running in your ECS service. For each task:

Task 1:

Frontend container: Exposed on host port 8080 (container port 80).
Backend container: Exposed on host port 9090 (container port 3000).
Task 2:

Frontend container: ECS might map this to host port 8081 (container port 80).
Backend container: ECS might map this to host port 9091 (container port 3000).
Task 3:

Frontend container: ECS might map this to host port 8082 (container port 80).
Backend container: ECS might map this to host port 9092 (container port 3000).

What This Means:
Host ports will be different for each task: ECS will ensure each task gets unique host ports (8080, 8081, 8082, etc.) to avoid conflicts it doesnt matter whats
metnoined in the task defin.
Task containers will always use the same internal ports: Inside each container, the ports are the same (port 80 for frontend, port 3000 for backend).


***********************************************************





Amazon ECS offers two distinct types of scaling: Service Autoscaling and Cluster Autoscaling. Here's a breakdown of the concepts,
how they work, and the tools and policies used.


1. Types of Scaling in ECS

A. Service Autoscaling
Focuses on tasks and containers that are running within the ECS service.
Utilizes Application Auto Scaling to dynamically adjust the desired count of tasks.
Scaling decisions are based on metrics like CPU or memory utilization, reported to Amazon CloudWatch.

Scaling Policies:
Target Tracking:
Automatically adjusts the number of running tasks to maintain a specific metric target.
Example: Keep CPU utilization at 50%.

Step Scaling:
Adjusts the number of tasks based on the severity of an alarm breach.
Larger breaches lead to larger adjustments in task count.

Scheduled Scaling:
Adjusts the desired count of tasks based on a predefined schedule.
Example: Scale up tasks at 9:00 AM daily to handle increased demand.


B. Cluster Autoscaling
Focuses on the ECS container instances (EC2 instances) that host the tasks.
Ensures sufficient resources are available for tasks to run by scaling the number of container hosts.

Uses:
Capacity Providers: ECS resources linked to autoscaling groups (ASGs) that manage EC2 instances.
EC2 Auto Scaling to dynamically increase or decrease the size of the cluster based on resource utilization.

Metrics Used:
Capacity Provider Reservation: Indicates the utilization of container instance resources (e.g., CPU or memory).


2. How Service Autoscaling Works

Setup:
An ECS service is deployed with tasks running on container instances.
Tasks report metrics like CPU or memory usage to Amazon CloudWatch.

Scaling Process:
When metrics (e.g., CPU utilization) exceed a defined threshold, an alarm triggers Application Auto Scaling.
Application Auto Scaling adjusts the desired task count in the ECS service.
ECS launches new tasks to meet the updated desired count.
These tasks are distributed across the available container instances.

Scaling Policies:
Target Tracking Scaling: Adjusts tasks to maintain a specific metric (e.g., 60% CPU utilization).
Step Scaling: Uses the size of metric breaches to decide the number of tasks to scale.
Scheduled Scaling: Scales tasks based on time-based rules (e.g., add 5 tasks at 8:00 AM daily).


3. How Cluster Autoscaling Works

Setup:
ECS cluster runs on EC2 instances managed by an Auto Scaling Group (ASG).
The ASG is linked to ECS using a Capacity Provider.
The Capacity Provider monitors resource usage and interacts with the ASG to scale container instances.

Scaling Process:
ECS tracks resource utilization metrics (e.g., CPU, memory) of the cluster's container instances.
When resources exceed a threshold (e.g., 80% CPU utilization), ECS reports the data to CloudWatch.
CloudWatch triggers the ASG to add a new EC2 instance.
The ECS service recognizes the new instance and schedules new tasks on it as needed.

Key Features:
Managed Scaling: Automatically adjusts the size of the ASG to ensure sufficient resources are available for tasks.
Instance Termination Protection:
Prevents instances from being terminated if tasks are still running on them.
Ensures graceful termination to avoid task disruptions.



4. Comparison of Service and Cluster Autoscaling

Feature	             Service Autoscaling	                      Cluster Autoscaling
Focus	               Scaling the number of running tasks.	      Scaling the number of container instances.
Key Resource	       Tasks in ECS service.	                    EC2 instances in ECS cluster.
Metrics	             Task-level metrics (e.g., CPU, memory).   	Cluster-level metrics (e.g., instance CPU).
Scaling Mechanism	   Application Auto Scaling.	                EC2 Auto Scaling using Capacity Providers.
Policies	           Targt Trackin, Step and Schduled Scalin.  	Managed Scaling and Instance Termination Protection.
TriggerExample	     High task CPU utilization.	                High container instance CPU utilization.



5. Visual Examples

Service Autoscaling Example:
An ECS cluster has 3 tasks running on container instances(is ec2 isntance).
Tasks report CPU utilization metrics to CloudWatch.
When the average CPU utilization exceeds 70%, CloudWatch triggers Application Auto Scaling.
Application Auto Scaling increases the desired count of tasks from 3 to 5.
ECS schedules and launches 2 additional tasks on the existing container instances.

Cluster Autoscaling Example:
An ECS cluster has 2 container instances in an Auto Scaling Group (ASG).
Tasks running on the instances report high resource utilization (e.g., 85% CPU).
CloudWatch triggers the ASG via the Capacity Provider.
The ASG adds a new EC2 instance to the cluster.
ECS recognizes the new instance and launches tasks on it to distribute the load.



6. Summary of Key Components

Application Auto Scaling:
Manages the scaling of tasks in ECS services (service autoscaling).
Capacity Providers:
Links ECS to ASGs for managing container instances (cluster autoscaling).
CloudWatch:
Monitors metrics like CPU and memory usage and triggers scaling actions.
Auto Scaling Groups (ASGs):
Dynamically adjusts the number of EC2 instances in the cluster.

Best Practices
Use Target Tracking for Simplicity:
Target tracking policies are easier to configure and manage compared to step scaling.

Implement Instance Termination Protection:
Prevent task disruptions by protecting instances with running tasks from being terminated.

Leverage Scheduled Scaling:
Optimize costs by scaling down during off-peak hours and scaling up during high-demand periods.

Monitor Metrics Regularly:
Continuously monitor CloudWatch metrics to fine-tune scaling thresholds.
These scaling mechanisms allow ECS to handle dynamic workloads efficiently, ensuring applications remain performant
while optimizing resource usage.










Load Balancer Connections and Container Awareness in ECS

This lesson explains how Application Load Balancers (ALBs) work with ECS services, focusing on container
awareness, networking, and traffic distribution.


1. Overview of the Architecture

Availability Zones:
ECS spans multiple Availability Zones (AZs) for high availability.
Each AZ has public and private subnets.

ECS Container Instances:
ECS container instances are EC2 instances that run containers.
Multiple ECS services can run on a single container instance.
Example: Two services (e.g., Nginx and Apache) may run on the same container instance.

Containers and Ports:
Each container runs a service, such as Nginx or Apache, on port 80 inside the container.
Host ports on the ECS container instance map incoming connections to the appropriate container.

***************



Key Points:

Tasks in ECS Service:
A service in ECS is responsible for maintaining a specified desired count of identical tasks running at all times.
The tasks in the service are usually based on the same task definition (which includes the same container image, resource requirements, environment variables, etc.).
If one task in the service fails, ECS will automatically launch a new task to replace it and maintain the desired count.

Pods in Kubernetes Service:
In Kubernetes, a service typically manages a set of identical pods that are also based on the same container image and configuration.
The Kubernetes service ensures that multiple replicas of a pod are running for high availability and scalability. If a pod fails, Kubernetes will 
reschedule a new one to maintain the desired number.

Example:
ECS Service Example:
You might have a service with a task definition that runs a web application container. You specify that you want 5 identical tasks running for your service.
ECS will run 5 copies of the task (each running the same application) across your container instances, ensuring the a
pplication is available even if one or more tasks fail.

Kubernetes Service Example:
You deploy a Kubernetes Deployment with a desired number of replicas (e.g., 3). Each replica is a pod running the same containerized application.
If one pod fails, Kubernetes will create a new one to maintain the desired replica count.

***************************



2. Host Ports and Container Awareness
Container Networking Challenge:
Containers running on the same ECS container instance may use the same port internally (e.g., port 80).
However, the host (EC2 instance) cannot have multiple services listening on the same port (e.g., port 80).

Dynamic Host Port Allocation:
ECS dynamically assigns a unique host port for each container.
The host port maps to the container's internal port (e.g., port 80).


3. Application Load Balancer (ALB) and Container Awareness
ALBâ€™s Role:
The ALB acts as the traffic distributor, forwarding connections to the correct container.
ALB is container-aware, meaning it understands the mapping between the host ports and container tasks.
How It Works:
The ALB listens on a single port (e.g., port 80) for incoming requests.
It uses the ECS service's dynamic host port mapping to forward connections to the appropriate container tasks.
Example:
Incoming traffic on port 80 is forwarded to a host port (e.g., port 32768).
The ECS container instance uses the host port to route the request to the correct container.


4. NAT Gateway and Internet Access
Private Subnets:
Containers running in private subnets cannot directly access the internet.
NAT Gateway:
A NAT gateway in the public subnet enables internet access for containers in private subnets.
Route Table Entry:
The private subnet's route table must have a route pointing to the NAT gateway for external traffic.

Use Cases:
Containers may need internet access to:
Download software dependencies or code.
Communicate with internet-based APIs.

5. Key Concepts
Feature	                   Explanation
Container Awareness	       ALBs understand the host-to-container port mapping and distribute traffic accordingly.
Host Ports	               Dynamically allocated ports on the ECS container instance, mapping traffic to container tasks.
NAT Gateway	               Provides internet access for containers in private subnets by routing traffic through the public subnet.
Private Subnets	           Containers in private subnets do not have direct internet access and require a NAT gateway.
Dynamic Traffic Routing	   ALB forwards requests from a single listener port (e.g., 80) to multiple dynamic host ports.


6. Visual Example

Incoming Request:
A user sends a request to the ALB on port 80.
Traffic Distribution:
ALB determines the host port mapped to the requested service.
Forwards the request to the specific host port (e.g., 32768) on the ECS container instance.
Container Handling:
The ECS container instance receives the request on the host port and routes it to the appropriate container.

7. Summary

Application Load Balancers:
Simplify traffic distribution for ECS tasks.
Automatically handle dynamic port mappings, enabling multiple services to run on the same container instance.

NAT Gateways:
Essential for containers in private subnets to access the internet.
Must be configured in the route table for private subnets.

Container Networking:
Host ports ensure unique traffic routing for containers running on the same instance.
This setup ensures efficient, scalable, and highly available containerized applications in ECS.








HANDS ON LESSON:


Deploying a Docker Container on AWS Fargate (ECS Serverless Implementation)

This lesson demonstrates how to deploy a Docker container on AWS Fargate, the serverless implementation of Amazon ECS, covering cluster creation, 
task definitions, task execution, and service setup.

1. Overview of Steps
Create a Cluster: Set up an ECS cluster with Fargate.
Define a Task: Create a task definition with container configurations.
Run a Task: Launch and verify the containerized application.(DEELTE THIS AND THE TRY DEPLYIn iwht services where we can run miltiple tasks)
Create a Service: Set up a service to maintain the desired number of tasks.
Clean Up: Delete tasks, services, and clusters to prevent unnecessary costs.


2. Detailed Steps
Step 1: Create a Cluster
Navigate to ECS Console:
Go to the Clusters section and click Create Cluster.
Cluster Configuration:
Choose the AWS Fargate launch type for serverless scaling.
Provide a name (e.g., my-fargate-cluster).
Click Create.
Note: If the cluster creation fails, delete the CloudFormation stack and retry.

Step 2: Create a Task Definition
>Navigate to Task Definitions: Click Task Definitions on the left and then Create Task Definition.

>Task Configuration:
Family Name: Provide a name (e.g., nginx-container).
Launch Type: Select Fargate.
CPU/Memory: Use defaults unless specific resource allocations are needed.
Task Roles: Skip unless the task needs API access to other AWS services.
Container Details:
Name: nginx-container.
Image: nginx:latest.
Port: 80 (for the web server).
Enable log collection for CloudWatch Logs.

>Create Task Definition.

Step 3: Run a Task
>Navigate to Cluster:
Select the created cluster and click Tasks â†’ Run New Task.

>Task Launch Settings:
Launch Type: Fargate.
Task Definition: Select the created task definition (e.g., nginx-container).
Networking:
VPC: Use the default VPC.
Subnet: Select a public subnet.
Security Group: Choose one with web access (e.g., allowing HTTP traffic on port 80).
Ensure Public IP is enabled.

>Launch Task:
Wait for the task to reach the Running state.
Verify by checking the task's Public IP under the Networking tab.
Enter the Public IP in a browser to see the default Nginx splash screen.

Step 4: Create a Service
>Purpose:
A service ensures the desired number of tasks (e.g., 2 tasks) are always running.
Enables features like load balancing and auto-scaling.
>Service Setup:
Navigate to the Services tab in the cluster.
Choose:
Launch Type: Fargate.
Task Definition: nginx-container.
Desired Task Count: 2.
Networking:
Use the same VPC, subnets, and security group as the task.
Enable Public IP.
Skip load balancing for now (optional).
>Create Service:
Verify 2 tasks are running in the service.

Step 5: Clean Up
>Stop and Delete Tasks:
Navigate to Tasks and stop any running tasks.
Delete the Service:
Navigate to Services, select the service, and delete it (use Force Delete if necessary).
>Delete the Cluster:
Ensure no tasks or services are running, then delete the cluster.

3. Key Concepts
Feature               	Explanation
AWS Fargate	            Serverless compute for ECS, no infrastructure management needed.
Task Definition	        Blueprint for running containers, including image, CPU/memory, and networking.
Task Roles	            Allow containers to make API requests to AWS services (if required).
Public IP and Security	Public IP and appropriate security groups are essential for web-accessible tasks.
Services	              Ensure a specific number of tasks are running, with options for scaling and load balancing.


4. Troubleshooting
Cluster Creation Fails:
Delete the CloudFormation stack and recreate the cluster with the same settings.

Task Stuck in Pending:
Verify correct VPC, subnet, and security group configurations.
Ensure the image repository is accessible (e.g., nginx:latest from DockerHub).

No Response from Public IP:
Ensure the security group allows inbound HTTP traffic on port 80.

5. Additional Notes

Load Balancing: Can be added during service creation for better traffic distribution.
Auto-Scaling: Enabled through metrics (e.g., CPU utilization) for dynamic task scaling.
Cost Management: Delete tasks, services, and clusters after use to avoid unnecessary charges.
This hands-on process simplifies containerized application deployment with Fargate's serverless capabilities.





















