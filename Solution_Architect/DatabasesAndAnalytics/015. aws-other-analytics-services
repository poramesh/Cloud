Analytics Services - Notes

1. Amazon Timestream
Purpose: A time-series database for IoT and operational applications.
Faster and cheaper than relational databases.
Automatically moves historical data to cost-optimized storage based on user-defined policies.
Serverless and scales automatically.
Use Case: Best for applications that need to store and analyze time-series data efficiently.

2. AWS Data Exchange
Purpose: A data marketplace with over 3,000 products from 250+ providers.
Supports: Data files, tables, and APIs.
Integration: Directly integrates with data lakes, applications, analytics, and ML models.
Automatically exports updated datasets to Amazon S3.
Can query datasets using Data Exchange for Redshift.
Use Case: Ideal for businesses that need external data sources for analytics, research, and ML models.

3. AWS Data Pipeline
Purpose: A managed ETL (Extract, Transform, Load) service.
Moves and processes data between AWS compute and storage services.
Data sources can be on-premises or cloud-based.
Can load processed data into S3, RDS, DynamoDB, and EMR.
Example Use Case:
Clickstream data is stored in S3.
HiveActivity converts log files to CSV using EMR.
The processed data is then copied to Redshift.
Use Case: Best for automating data workflows and handling ETL processes efficiently.

4. Data Lakes vs Data Warehouses

Feature	          Data Warehouse	                                           Data Lake
Data Type	        Relational data from transactional systems	               Structured + unstructured data from IoT, websites, mobile apps
Schema 	          Schema-on-write (Defined before storing data)	             Schema-on-read (Defined when analyzing data)
Performance	      Fast queries with high-cost storage	Lower-cost             storage with improving query performance
Data Quality	    Highly curated, "single version of truth"	                 Raw data, may not be curated
Users	            Business analysts	                                          Data scientists, developers, analysts
Use Cases	        Business intelligence, batch reporting, dashboards	        Machine learning, predictive analytics, big data processing


5. AWS Lake Formation
Purpose: Helps create secure data lakes quickly (in days, not months).
Data Sources: Collects data from databases, object storage (S3), etc.
Processing: Cleans and classifies data using ML algorithms.
Security: Supports column, row, and cell-level security.
Integrations: Works with Redshift, Athena, EMR, Apache Spark, and QuickSight.
Built on AWS Glue capabilities.
Use Case: Best for companies needing centralized big data storage with strong security and governance.

6. Amazon Managed Streaming for Apache Kafka (MSK)
Purpose: A fully managed service for ingesting and processing streaming data using Apache Kafka.
Alternative to AWS Kinesis, but based on open-source Kafka instead of AWS-specific tech.
Automates cluster provisioning and maintenance (including Apache Zookeeper nodes).
Security Features:
VPC network isolation
IAM-based control access
Encryption at rest and in transit (TLS)
TLS-based certificate authentication
SASL and SCRAM authentication via AWS Secrets Manager
Use Case: Best for real-time data streaming and processing at scale, especially when using Kafka-based applications.

Exam Tips
If a question mentions "time-series data" → Think of Timestream.
If it involves "buying or selling data" → Think of Data Exchange.
If it's about "moving and transforming data (ETL)" → Think of Data Pipeline.
If it's about "large-scale raw data storage" → Think of Lake Formation.
If it's about "real-time streaming data with Kafka" → Think of MSK.
