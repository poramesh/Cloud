Analytics Services - Notes

1. Amazon Timestream
Purpose: A time-series database for IoT and operational applications.
Faster and cheaper than relational databases.
Automatically moves historical data to cost-optimized storage based on user-defined policies.
Serverless and scales automatically.
Use Case: Best for applications that need to store and analyze time-series data efficiently.

2. AWS Data Exchange
Purpose: A data marketplace with over 3,000 products from 250+ providers.
Supports: Data files, tables, and APIs.
Integration: Directly integrates with data lakes, applications, analytics, and ML models.
Automatically exports updated datasets to Amazon S3.
Can query datasets using Data Exchange for Redshift.
Use Case: Ideal for businesses that need external data sources for analytics, research, and ML models.

3. AWS Data Pipeline
Purpose: A managed ETL (Extract, Transform, Load) service.
Moves and processes data between AWS compute and storage services.
Data sources can be on-premises or cloud-based.
Can load processed data into S3, RDS, DynamoDB, and EMR.
Example Use Case:
Clickstream data is stored in S3.
HiveActivity converts log files to CSV using EMR.
The processed data is then copied to Redshift.
Use Case: Best for automating data workflows and handling ETL processes efficiently.

4. Data Lakes vs Data Warehouses

Feature	          Data Warehouse	                                           Data Lake
Data Type	        Relational data from transactional systems	               Structured + unstructured data from IoT, websites, mobile apps
Schema 	          Schema-on-write (Defined before storing data)	             Schema-on-read (Defined when analyzing data)
Performance	      Fast queries with high-cost storage	Lower-cost             storage with improving query performance
Data Quality	    Highly curated, "single version of truth"	                 Raw data, may not be curated
Users	            Business analysts	                                          Data scientists, developers, analysts
Use Cases	        Business intelligence, batch reporting, dashboards	        Machine learning, predictive analytics, big data processing


5. AWS Lake Formation
Purpose: Helps create secure data lakes quickly (in days, not months).
Data Sources: Collects data from databases, object storage (S3), etc.
Processing: Cleans and classifies data using ML algorithms.
Security: Supports column, row, and cell-level security.
Integrations: Works with Redshift, Athena, EMR, Apache Spark, and QuickSight.
Built on AWS Glue capabilities.
Use Case: Best for companies needing centralized big data storage with strong security and governance.

6. Amazon Managed Streaming for Apache Kafka (MSK)
Purpose: A fully managed service for ingesting and processing streaming data using Apache Kafka.
Alternative to AWS Kinesis, but based on open-source Kafka instead of AWS-specific tech.
Automates cluster provisioning and maintenance (including Apache Zookeeper nodes).
Security Features:
VPC network isolation
IAM-based control access
Encryption at rest and in transit (TLS)
TLS-based certificate authentication
SASL and SCRAM authentication via AWS Secrets Manager
Use Case: Best for real-time data streaming and processing at scale, especially when using Kafka-based applications.

Exam Tips
If a question mentions "time-series data" → Think of Timestream.
If it involves "buying or selling data" → Think of Data Exchange.
If it's about "moving and transforming data (ETL)" → Think of Data Pipeline.
If it's about "large-scale raw data storage" → Think of Lake Formation.
If it's about "real-time streaming data with Kafka" → Think of MSK.






AWS Glue – Serverless ETL and Data Catalog

✅ Best for: Automated ETL (Extract, Transform, Load) for analytics

Key Features:
Serverless → No need to manage infrastructure.
Uses Apache Spark for large-scale data processing.
Automates schema discovery with Glue Crawlers.
Provides a managed Data Catalog (metadata for Athena, Redshift, EMR, etc.).
Supports real-time and batch data processing.

Common Use Cases:
✔ Running ETL jobs on S3, RDS, Redshift, DynamoDB data.
✔ Creating Athena tables from raw S3 data.
✔ Automating data transformations (cleaning, deduplication).
✔ Integrating with Lake Formation for data governance.


 AWS Data Pipeline – Orchestration for Data Workflows
✅ Best for: Automating Data Movement & Scheduling Workflows

Key Features:
Used for data movement between AWS services (e.g., S3 → Redshift).
Allows scheduling & dependency management (e.g., run job after another finishes).
Supports batch processing (not real-time).
Manages workflow execution (e.g., trigger jobs based on time/events).
Can run custom scripts (EC2, EMR, on-premise servers).

Common Use Cases:
✔ Copying data from S3 to Redshift or RDS.
✔ Scheduling ETL workflows across multiple AWS services.
✔ Running custom scripts on EC2/EMR at specific intervals.
✔ Moving on-premise data to AWS storage.

Key Differences Table
Feature	                  AWS Glue	                          AWS Data Pipeline
Purpose	                  ETL, Data Catalog, Transformations	Data movement, Workflow automation
Serverless?	              ✅ Yes	                            ❌ No (requires EC2/EMR for processing)
Compute Engine	          Apache Spark	                      EC2/EMR
Real-time processing    	✅ Yes (near real-time)	            ❌ No (batch-based)
Workflow orchestration	  ❌ No (limited job scheduling)	    ✅ Yes (triggers & dependencies)
Best for                	Data transformation, analytics	    Scheduling and moving data


When to Use What?
Use AWS Glue if you need serverless ETL, schema discovery, and data cataloging.
Use AWS Data Pipeline if you need data movement workflows and scheduling across AWS
